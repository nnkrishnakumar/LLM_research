{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding:Word embeddings are used in natural language processing (NLP) due to their ability to capture the semantic meaning of words. They map words to a high-dimensional semantic space,\n",
    "# where words that are used in similar contexts are given similar representations. This means that words used in similar ways are placed close together within the high-dimensional semantic space, \n",
    "# clustering together and having low distances from each other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# handpicked sample data to understand the working of supervise word embedding\n",
    "\n",
    "reviews=[\"nice work\",\"work well\",\"nice work\",\"amazing project\",\"horrible work\",\"awesome task welldone\"]                   # corpus: it is the list of all the document(document are the number of string available inside the corpus(list))\n",
    "sentiments=[\"positive\",\"positive\",\"positive\",\"positive\",\"negative\",\"positive\"]\n",
    "\n",
    "# conversion of sentiment into integer as per classes count\n",
    "sentiments_num_class=[1 if i==\"negative\" else 0 for i in sentiments]\n",
    "# conversion of sentiments_num_class into array\n",
    "sentiment_array=np.array(sentiments_num_class)\n",
    "sentiment_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 20]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoding Test: to convert this number to encoding with one document later we will encode all document into vector\n",
    "one_hot(\"nice work\",30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 20], [20, 3], [13, 20], [8, 5], [19, 20], [8, 25, 22]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding all document using one hote encoding\n",
    "vocab_size=30                                           # it can be anything (Note: vocabulary size always grater the the number of differnt word in the document given above)\n",
    "reviews_encode=[one_hot(i,vocab_size) for i in reviews]\n",
    "reviews_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[47, 33], [33, 47], [47, 33], [2, 4], [38, 33], [2, 26, 16]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we want then we can also change the vocab size \n",
    "# encoding all document using one hote encoding\n",
    "vocab_size=60                                          # it can be anything (Note: vocabulary size always grater the the number of differnt word in the document given above)\n",
    "reviews_encode=[one_hot(i,vocab_size) for i in reviews]\n",
    "reviews_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[47, 33,  0,  0],\n",
       "       [33, 47,  0,  0],\n",
       "       [47, 33,  0,  0],\n",
       "       [ 2,  4,  0,  0],\n",
       "       [38, 33,  0,  0],\n",
       "       [ 2, 26, 16,  0]], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now max length of a the biggest document and count the number of string available in it. Always add 1 to the len(of the biggest string of the corpus): exmaple :::>>>\"awesome task welldone\" it has 3 string when you will tokenize or split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "max_len=4\n",
    "padding_reviews=pad_sequences(reviews_encode,maxlen=max_len,padding='post')    # padding=\"post\"  is used to put 0 at the end to make all vector of same dimension.\n",
    "padding_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 05:22:02.391234: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# create model \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,Flatten,Dense\n",
    "\n",
    "embedding_vector_size=10\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,embedding_vector_size,input_length=max_len,name=\"embedding\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem face by those machine who doesnot have GPU in their machin. || The above code is giving us a problem because above code is using GPU and GPU is use by CUDA \n",
    "# Solution: with tensorflow library only ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"     # this is running your code on CPU not on GPU now.\n",
    "\n",
    "# Now run all the above code \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,Flatten,Dense\n",
    "\n",
    "embedding_vector_size=10\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,embedding_vector_size,input_length=max_len,name=\"embedding\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create feature and target and compile the model \n",
    "\n",
    "X= padding_reviews\n",
    "Y= sentiment_array\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 10)             600       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 40)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 641 (2.50 KB)\n",
      "Trainable params: 641 (2.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# now to see the model summary use \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.2860 - accuracy: 0.8333\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2842 - accuracy: 0.8333\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2825 - accuracy: 0.8333\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2808 - accuracy: 0.8333\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2791 - accuracy: 0.8333\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2774 - accuracy: 0.8333\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2758 - accuracy: 0.8333\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2741 - accuracy: 0.8333\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2724 - accuracy: 0.8333\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2708 - accuracy: 0.8333\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2691 - accuracy: 0.8333\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2675 - accuracy: 0.8333\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2658 - accuracy: 0.8333\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2642 - accuracy: 0.8333\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2626 - accuracy: 0.8333\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2610 - accuracy: 0.8333\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2593 - accuracy: 0.8333\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2577 - accuracy: 0.8333\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2561 - accuracy: 0.8333\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2546 - accuracy: 0.8333\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2530 - accuracy: 0.8333\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2514 - accuracy: 0.8333\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2498 - accuracy: 0.8333\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2483 - accuracy: 0.8333\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2467 - accuracy: 0.8333\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2452 - accuracy: 0.8333\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2436 - accuracy: 0.8333\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2421 - accuracy: 0.8333\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2406 - accuracy: 0.8333\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2391 - accuracy: 0.8333\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2376 - accuracy: 0.8333\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2360 - accuracy: 0.8333\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2345 - accuracy: 0.8333\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2331 - accuracy: 0.8333\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2316 - accuracy: 0.8333\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2301 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2286 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2272 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2257 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2242 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2228 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2214 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2199 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2185 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2171 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2157 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2143 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2129 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2115 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2101 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9257fca230>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now train the model\n",
    "\n",
    "model.fit(X,Y,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step - loss: 0.2087 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.208733931183815, 1.0]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluating the model\n",
    "model.evaluate(X,Y)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2087 - accuracy: 1.0000\n",
      "accuracy : 1.0\n"
     ]
    }
   ],
   "source": [
    "# store loss and accuracy and unpack the above model.evaluates(X,Y)\n",
    "\n",
    "loss,accuracy=model.evaluate(X,Y)\n",
    "print(\"accuracy :\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.81481987e-01  9.81912017e-02 -8.11286569e-02  1.84130087e-01\n",
      "   1.97620824e-01  1.67721063e-01 -1.32870883e-01  1.85122252e-01\n",
      "   1.70728192e-01  1.76468343e-01]\n",
      " [ 4.10062335e-02  1.67233385e-02 -1.41904727e-02  4.27175499e-02\n",
      "   1.54097117e-02 -4.17105928e-02  6.76435232e-03  1.44226886e-02\n",
      "   6.20497391e-03 -9.20940191e-04]\n",
      " [-1.80071592e-01  1.45849288e-01 -1.30931169e-01  2.10702717e-01\n",
      "  -1.67756483e-01  1.50376394e-01 -7.63800666e-02  1.74155623e-01\n",
      "   8.46491903e-02 -2.21624851e-01]\n",
      " [-1.36000291e-02  3.32340114e-02 -1.09912641e-02 -1.16771087e-02\n",
      "  -4.58547249e-02 -1.25338659e-02 -4.78052497e-02  3.58204357e-02\n",
      "  -2.46064197e-02 -2.25553513e-02]\n",
      " [ 2.03014731e-01  2.30629683e-01  2.14637324e-01 -1.56024486e-01\n",
      "  -2.26284966e-01  1.55017376e-01 -1.60304308e-01  1.60930812e-01\n",
      "  -1.86613575e-01 -1.96894199e-01]\n",
      " [ 4.84451391e-02  3.92975658e-03  4.29464839e-02  4.49511670e-02\n",
      "  -2.13770624e-02 -2.89131999e-02 -1.98503379e-02 -1.36592016e-02\n",
      "   3.20086814e-02  4.74194996e-02]\n",
      " [ 4.65491749e-02  4.94098775e-02  2.68808343e-02  3.89344357e-02\n",
      "  -3.74367461e-02 -3.35070118e-02  3.12663950e-02 -2.27995869e-02\n",
      "   3.48243862e-03  2.23284476e-02]\n",
      " [ 9.95947048e-03 -3.25172171e-02 -1.16896741e-02 -2.84059048e-02\n",
      "   3.68179120e-02 -2.93033011e-02 -4.84619513e-02  1.55042075e-02\n",
      "   2.07531713e-02 -3.49607468e-02]\n",
      " [-4.64826822e-02  3.29846404e-02  2.43017562e-02  1.56930722e-02\n",
      "  -1.26689784e-02 -2.60348078e-02  3.56550850e-02 -1.74181350e-02\n",
      "  -2.05937754e-02  4.87334169e-02]\n",
      " [-4.36831824e-02 -6.52800873e-03  1.35058798e-02 -8.68135691e-03\n",
      "   4.75313514e-03 -1.49906501e-02 -2.20618844e-02 -1.60452612e-02\n",
      "   4.89156730e-02  9.95188951e-03]\n",
      " [ 2.90547721e-02 -2.47104298e-02 -5.78834862e-03 -8.44019651e-03\n",
      "  -5.33571094e-03 -1.07399113e-02  4.50543426e-02  1.11617930e-02\n",
      "  -4.53069471e-02 -4.00744379e-04]\n",
      " [ 3.55616957e-03  3.22037600e-02 -2.53244173e-02 -3.91387716e-02\n",
      "   4.04229052e-02 -1.37662664e-02 -1.99402329e-02 -4.21205536e-02\n",
      "  -4.86835502e-02 -8.50436836e-03]\n",
      " [-1.11508369e-03 -2.39639524e-02 -4.31895033e-02  9.01653618e-03\n",
      "  -1.60607807e-02  1.24548748e-03  2.94646509e-02  3.86811830e-02\n",
      "  -4.10348177e-03  3.18478830e-02]\n",
      " [-4.14882302e-02  1.27913691e-02  2.21540593e-02  3.23919915e-02\n",
      "  -1.07597597e-02  1.01004839e-02 -4.57202904e-02 -5.00740856e-03\n",
      "   9.66180488e-03 -4.14088853e-02]\n",
      " [-1.78779848e-02  3.70453112e-02  2.35488676e-02 -4.42767031e-02\n",
      "  -1.98268183e-02 -3.98748741e-02 -1.32388249e-02 -1.47149339e-02\n",
      "   4.44238223e-02  6.16387278e-03]\n",
      " [ 8.07895511e-03  4.91621345e-03  9.21884924e-03 -4.87932079e-02\n",
      "   2.15868242e-02  2.83153318e-02  4.46892641e-02  7.28831440e-03\n",
      "   4.87983935e-02 -2.14419514e-03]\n",
      " [-7.62103125e-02  1.93326712e-01 -1.90296263e-01  2.16960683e-01\n",
      "   1.56846434e-01  2.49450386e-01  1.33463070e-01 -1.60909653e-01\n",
      "   1.33954793e-01 -4.84479181e-02]\n",
      " [-4.83626127e-02  1.42764486e-02  4.02986296e-02 -2.75588166e-02\n",
      "  -2.06735861e-02  4.90778685e-03  4.25698273e-02 -1.60496235e-02\n",
      "   4.88045327e-02  4.77086380e-03]\n",
      " [ 4.92230393e-02 -3.71277556e-02  4.99441288e-02 -4.83429544e-02\n",
      "   1.92729346e-02 -8.17690045e-03 -9.44728777e-03 -4.88947295e-02\n",
      "  -4.56760079e-03  3.04159634e-02]\n",
      " [-4.35771942e-02  2.64967419e-02 -1.98933128e-02 -1.62725933e-02\n",
      "   7.20751286e-03 -1.41672604e-02 -2.00000163e-02  1.86714791e-02\n",
      "   3.55621912e-02 -3.42510641e-04]\n",
      " [-3.29426751e-02  1.42960884e-02  4.76020463e-02  2.63343342e-02\n",
      "   4.36066277e-02 -4.03637774e-02  7.56788999e-04  1.90104172e-03\n",
      "   4.08006050e-02  2.93787159e-02]\n",
      " [ 3.19018699e-02 -3.05842403e-02  4.05274071e-02 -8.03971291e-03\n",
      "   4.78129461e-03  2.96432413e-02  9.59814712e-03  3.83595265e-02\n",
      "   2.16787569e-02 -4.55155037e-02]\n",
      " [ 1.27308704e-02 -1.06223449e-02 -3.72205377e-02 -2.53449921e-02\n",
      "   3.84022333e-02 -4.92613576e-02  1.75643601e-02  4.64937724e-02\n",
      "  -1.13409273e-02  3.76710184e-02]\n",
      " [ 4.72948290e-02  7.16205686e-03  2.43189968e-02  1.01815350e-02\n",
      "  -4.46236618e-02 -9.72177833e-03 -7.34640285e-03 -2.42091063e-02\n",
      "   2.32088901e-02 -4.28999439e-02]\n",
      " [ 4.60928343e-02 -1.86862834e-02 -2.02227384e-04 -4.89568226e-02\n",
      "   3.62411477e-02 -4.57517058e-03 -2.94565205e-02 -2.99350861e-02\n",
      "   3.90497334e-02 -1.76172741e-02]\n",
      " [ 4.15429212e-02 -4.27800417e-02  2.64319070e-02 -1.74633861e-02\n",
      "   2.08596103e-02 -4.60567363e-02 -4.17500846e-02 -2.68398523e-02\n",
      "  -2.95795798e-02 -4.99758609e-02]\n",
      " [ 2.04232723e-01  2.24964336e-01  2.48618767e-01 -2.12938130e-01\n",
      "  -1.54863536e-01  1.27083391e-01 -1.52190194e-01  1.90336317e-01\n",
      "  -1.69544920e-01 -1.75759584e-01]\n",
      " [-2.46538166e-02 -2.85168178e-02 -2.57045161e-02  4.92360853e-02\n",
      "   2.06089281e-02  2.02462189e-02 -1.21706128e-02  1.84740461e-02\n",
      "  -1.21513717e-02 -5.33279032e-03]\n",
      " [ 4.82643954e-02 -2.48454511e-04 -2.25066897e-02  2.93313339e-03\n",
      "   3.74459513e-02 -2.89168712e-02 -4.96281274e-02  3.90486792e-03\n",
      "  -1.61039941e-02  5.37933037e-03]\n",
      " [-4.37630340e-03  3.19048055e-02 -4.99155186e-02 -4.86946367e-02\n",
      "   3.27538736e-02  1.80765428e-02 -7.06810877e-03 -2.52350923e-02\n",
      "  -2.01997645e-02 -3.06795239e-02]\n",
      " [ 3.51999439e-02  4.64195125e-02 -4.62858789e-02 -8.95956904e-03\n",
      "  -5.95591962e-04  2.59997882e-02 -4.17265296e-02 -2.39792224e-02\n",
      "   1.68876909e-02 -2.09655892e-02]\n",
      " [ 8.86342674e-03  1.02302656e-02 -6.63487986e-03 -4.95478176e-02\n",
      "   1.44639723e-02 -4.54998016e-02  1.20848194e-02  4.87564318e-02\n",
      "   6.52610138e-03 -5.41479513e-03]\n",
      " [ 4.16336097e-02  1.83639266e-02 -1.19288191e-02  4.51872982e-02\n",
      "   2.65410654e-02  3.03304903e-02 -1.76238529e-02 -2.67449375e-02\n",
      "   3.53338383e-02 -2.88757570e-02]\n",
      " [-3.15846831e-01  8.70364532e-02 -1.92866042e-01  2.75303096e-01\n",
      "  -1.61770910e-01  3.18701752e-02 -2.14157552e-02  1.40691608e-01\n",
      "  -3.10425498e-02 -1.58787027e-01]\n",
      " [ 4.25447114e-02  3.23786251e-02 -1.20283738e-02  5.40994108e-04\n",
      "  -4.33769114e-02  4.14233543e-02  2.86358260e-02 -1.29710510e-03\n",
      "   2.31826417e-02 -3.23438272e-02]\n",
      " [ 3.54618691e-02 -6.77957386e-03 -4.06940468e-02  6.14746660e-03\n",
      "   4.86130156e-02 -2.02337392e-02 -1.55073777e-02  3.77084874e-02\n",
      "  -1.60148367e-02 -2.81872638e-02]\n",
      " [ 1.27739049e-02 -4.41245325e-02 -1.79655664e-02 -6.62323087e-03\n",
      "  -1.50505677e-02  1.30521692e-02  1.91265680e-02  2.64791287e-02\n",
      "   3.52738053e-03  4.60565351e-02]\n",
      " [-3.40183377e-02  1.48486383e-02 -3.70102637e-02  8.08215141e-03\n",
      "  -2.62480732e-02  8.24167579e-03 -4.27032225e-02 -1.31037086e-03\n",
      "  -1.69308297e-02  3.28171514e-02]\n",
      " [ 2.76353449e-01 -1.38836637e-01  2.79608816e-01 -2.97307491e-01\n",
      "   2.19554663e-01 -2.81190068e-01  1.42522112e-01 -2.48881400e-01\n",
      "  -8.35496411e-02  2.92507172e-01]\n",
      " [-2.35612877e-02  3.43316793e-03  8.22079182e-03  4.65601794e-02\n",
      "   2.60137059e-02  4.69760187e-02  1.30119585e-02 -7.90894032e-03\n",
      "  -2.03291178e-02  4.78034951e-02]\n",
      " [-4.29395586e-03 -2.95631289e-02  2.41619088e-02  3.36593129e-02\n",
      "  -4.16266918e-03  1.39796473e-02 -4.33137529e-02  5.31845167e-03\n",
      "  -2.97602415e-02 -3.69256847e-02]\n",
      " [ 4.40683849e-02 -1.87600981e-02  7.66299665e-04  4.30537350e-02\n",
      "  -3.70659679e-03  3.40103246e-02  3.95171717e-03  3.33408266e-03\n",
      "   3.99369337e-02  1.54189020e-03]\n",
      " [-1.15603581e-02 -3.97593603e-02 -4.13113348e-02  2.55900957e-02\n",
      "   4.87984307e-02  8.25983286e-03 -3.48650441e-02 -1.54339895e-02\n",
      "  -1.50358565e-02 -9.82701778e-03]\n",
      " [-1.57710314e-02  3.19484137e-02  1.83229707e-02 -4.67087999e-02\n",
      "  -2.96558626e-02  4.01809476e-02 -2.11948641e-02  4.93615009e-02\n",
      "  -2.75161397e-02  2.36354582e-02]\n",
      " [-1.11196265e-02 -7.96973705e-04  1.70262568e-02 -3.18825468e-02\n",
      "  -1.07238777e-02 -3.19677964e-02 -1.71553120e-02 -4.26781178e-03\n",
      "  -2.88210995e-02  8.45648348e-04]\n",
      " [ 2.25147940e-02 -1.15626454e-02  1.97233222e-02  4.64408286e-02\n",
      "  -4.20967340e-02  1.33518688e-02 -3.22025903e-02  4.28898372e-02\n",
      "  -3.58432308e-02  4.87327315e-02]\n",
      " [-2.62381434e-02  2.31815092e-02  3.12622637e-03 -5.70704788e-03\n",
      "   3.97877209e-02 -3.73151191e-02 -2.37085670e-03 -1.96345448e-02\n",
      "   4.60513681e-03  4.89625223e-02]\n",
      " [-1.80683345e-01  3.01738471e-01 -1.78339735e-01  1.72650039e-01\n",
      "  -1.78177282e-01  2.65295416e-01 -2.38976821e-01  1.89296350e-01\n",
      "  -2.64497519e-01 -1.52527004e-01]\n",
      " [ 3.14846672e-02  2.31298059e-03 -3.32176685e-02 -2.34602764e-03\n",
      "  -3.81756909e-02 -2.77592894e-02 -1.57970302e-02 -3.54903452e-02\n",
      "  -4.28854339e-02  2.01125778e-02]\n",
      " [-2.96788346e-02  9.62503999e-03 -4.34436649e-03  4.55763452e-02\n",
      "  -1.98160764e-02 -2.08479762e-02  2.28916667e-02  3.40449549e-02\n",
      "   4.30695675e-02 -2.63179298e-02]\n",
      " [-3.25574279e-02 -4.28510308e-02  4.73938026e-02 -1.34852529e-02\n",
      "   2.97846533e-02  1.52923353e-02 -2.00385936e-02 -1.82064883e-02\n",
      "  -8.32989067e-03  4.31952626e-03]\n",
      " [-3.02996524e-02  1.47193931e-02  2.70836614e-02 -1.32691152e-02\n",
      "   1.64352544e-02  3.47873233e-02  4.81277704e-03 -4.85175028e-02\n",
      "   1.09521635e-02 -3.71847525e-02]\n",
      " [ 2.63273977e-02 -4.63700406e-02 -2.88979057e-02  1.41137503e-02\n",
      "  -4.98347059e-02  3.94516252e-02 -2.05154307e-02  4.77414206e-03\n",
      "  -4.47646864e-02  1.31367557e-02]\n",
      " [-2.13634502e-02  1.48287453e-02  1.11680031e-02  2.37761028e-02\n",
      "  -3.85659933e-02 -2.47654449e-02  3.08948793e-02 -4.76116054e-02\n",
      "   8.83467123e-03  2.95679606e-02]\n",
      " [-2.81349309e-02  7.64467567e-03 -5.29184192e-03 -1.40703805e-02\n",
      "   4.27817814e-02 -3.64729874e-02  4.96963151e-02  4.19180058e-02\n",
      "   4.29989360e-02  4.06838581e-03]\n",
      " [ 2.30043568e-02 -2.61901971e-02  3.63079347e-02  3.00030150e-02\n",
      "  -5.14913723e-03  2.18101628e-02 -1.81467645e-02 -3.51867080e-02\n",
      "   4.72077616e-02  1.63442008e-02]\n",
      " [ 1.16588585e-02  2.75189616e-02 -1.02971420e-02 -1.13443509e-02\n",
      "   6.34945929e-04  1.53826587e-02 -1.27754435e-02 -5.29487059e-03\n",
      "   3.02793421e-02  4.69663627e-02]\n",
      " [-3.52179892e-02 -6.97199255e-03  4.75283749e-02 -4.60701101e-02\n",
      "  -3.74329090e-02 -6.92355633e-03  1.47126056e-02 -4.07291763e-02\n",
      "  -4.35462371e-02 -3.54832299e-02]\n",
      " [-2.27754191e-03 -1.98546778e-02  4.30682041e-02  4.70211022e-02\n",
      "  -2.86530852e-02  1.96800269e-02 -4.99143973e-02 -2.44799387e-02\n",
      "  -1.34387240e-02  3.72600220e-02]\n",
      " [-4.92737889e-02  4.41875942e-02 -3.51110324e-02  8.62956047e-04\n",
      "  -7.05766678e-03  4.98809852e-02 -8.12823698e-03 -4.82481606e-02\n",
      "  -1.00506060e-02 -4.91530672e-02]]\n"
     ]
    }
   ],
   "source": [
    "weight=model.get_layer(\"embedding\").get_weights()[0]\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.get_layer(\"embedding\").get_weights()[0]\n",
    "\n",
    "ANSWER \n",
    "The expression model.get_layer(\"embedding\").get_weights()[0](https://stackoverflow.com/questions/54430475/keras-embedding-weights-lookup-with-categorical-variables) is used to extract the weights of the embedding layer from a trained model in TensorFlow. \n",
    "\n",
    "Let's break down what each part does:\n",
    "\n",
    "model.get_layer(\"embedding\"): This retrieves the layer named \"embedding\" from the model. In the context of natural language processing, this usually refers to the embedding layer, which learns word embeddings from the input data.\n",
    "\n",
    ".get_weights(): This method returns the weights of the layer as a list of Numpy arrays. For an embedding layer, this will return a list with one element: the weight matrix of the embeddings. The weight matrix has a shape of (vocab_size, embedding_dimension), where vocab_size is the size of the vocabulary (i.e., the number of unique words in the input data), and embedding_dimension is the size of the embedding vectors.\n",
    "[ [0] ](https://stackoverflow.com/questions/54430475/keras-embedding-weights-lookup-with-categorical-variables): This is indexing the list returned by get_weights(). Since there is only one element in the list (the weight matrix), we access it with index 0.\n",
    "So, overall, model.get_layer(\"embedding\").get_weights()[ [0] ](https://stackoverflow.com/questions/54430475/keras-embedding-weights-lookup-with-categorical-variables) gives us the weight matrix of the embedding layer, which contains the word embeddings learned from the training data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Most important it is not as much efficient as infront of word2Vec using unsupervise learning/self supervise technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
